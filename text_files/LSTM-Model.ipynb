{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import tensorflow as tf\n",
    "import json\n",
    "#import tensorflow_addons as tfa\n",
    "import tensorflow.compat.v1 as tf1\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.activations import *\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Other imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setting verbosity level of tensorflow training\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "#from pandas_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Dropout,BatchNormalization, Embedding, LSTM, GRU\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine that shit\n",
    "# # # import glob\n",
    "# # os.chdir(\"./Kickstarter/\")\n",
    "# # extension = 'csv'\n",
    "# # all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "# # #combine all files in the list\n",
    "# # combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "# # #export to csv\n",
    "# # combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# #Remove Duplicates\n",
    "# # df = pd.read_csv('./Cleaned_data')\n",
    "# # df = df.drop_duplicates(subset=['id'])\n",
    "# # df.to_csv(\"Cleaned_data.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# # New Files\n",
    "# df = pd.read_csv('./Cleaned_data.csv')\n",
    "# df = df.drop(columns=['backers_count', 'country', 'country_displayable_name', 'created_at', 'creator', 'currency_symbol', 'currency_trailing_code', 'current_currency',\n",
    "#                       'disable_communication', 'friends', 'fx_rate', 'id', 'is_backing', 'is_starrable', 'is_starred', 'permissions', 'photo', 'slug', 'source_url', 'spotlight',\n",
    "#                       'staff_pick', 'state_changed_at', 'static_usd_rate', 'urls', 'usd_type', 'usd_exchange_rate', 'usd_pledged', 'profile', 'pledged', 'location', 'currency',\n",
    "#                       'converted_pledged_amount'], axis=1)\n",
    "\n",
    "# df['launch_to_deadline_days'] = round((df['deadline'] - df['launched_at'])/86400, 0)\n",
    "# df = df.drop(columns=['deadline', 'launched_at'], axis=1)\n",
    "\n",
    "# df['blurb_len'] = df['blurb'].map(str).apply(len)\n",
    "# df = df.drop(columns=['blurb'], axis=1)\n",
    "\n",
    "# df['name_len'] = df['name'].map(str).apply(len)\n",
    "# df = df.drop(columns=['name'], axis=1)\n",
    "\n",
    "# df['temp_cat'] = df['category'].apply(json.loads)\n",
    "# df2 = df.temp_cat.apply(pd.Series)\n",
    "# df['category'] = df2['parent_name']\n",
    "# df = df.drop(columns='temp_cat', axis=1)\n",
    "# df = df.drop(df[df['state'] == 'canceled'].index)\n",
    "# df = df.drop(df[df['state'] == 'live'].index)\n",
    "# df = df[['name_len', 'blurb_len', 'goal', 'launch_to_deadline_days', 'category', 'state']]\n",
    "# df.loc[df['state'] == 'successful', 'SuccessfulBool'] = 1\n",
    "# df.loc[df['state'] == 'failed', 'SuccessfulBool'] = 0\n",
    "# df = df.drop(columns='state')\n",
    "\n",
    "# # Old Files\n",
    "# df1 = pd.read_csv('./kickstarter_data_full.csv')\n",
    "# df1 = df1.drop(df1[df1['state'] == 'canceled'].index)\n",
    "# df1 = df1.drop(df1[df1['state'] == 'live'].index)\n",
    "# df1 = df1.drop(df1[df1['state'] == 'suspended'].index)\n",
    "# df1 = df1.drop(df1[df1['TOPCOUNTRY'] == 0].index)\n",
    "# df1 = df1[['name_len', 'blurb_len', 'goal', 'launch_to_deadline_days', 'category', 'SuccessfulBool']]\n",
    "\n",
    "# # Concat\n",
    "# df = pd.concat([df, df1])\n",
    "\n",
    "# # Data/Target prep\n",
    "y = df.SuccessfulBool\n",
    "X = df[['name_len', 'blurb_len', 'goal', 'launch_to_deadline_days', 'category']]\n",
    "# X['category'] = df['category'].astype('category')\n",
    "# X['category'] = X['category'].cat.codes\n",
    "X = pd.get_dummies(X, columns=['category'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import backend as k\n",
    "# class CustomCallback(tf.keras.callbacks.Callback):\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         if (logs.get('val_acc') + 5) < logs.get('acc')  :\n",
    "#             self.model.stop_training = True\n",
    "#         # gc.collect()\n",
    "#         # k.clear_session()\n",
    "# my_callback_object = CustomCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "  # keras.metrics.TruePositives(name='tp'),\n",
    "  # keras.metrics.FalsePositives(name='fp'),\n",
    "  # keras.metrics.TrueNegatives(name='tn'),\n",
    "  # keras.metrics.FalseNegatives(name='fn'), \n",
    "  keras.metrics.Precision(name='precision'),\n",
    "  keras.metrics.Recall(name='recall'),\n",
    "  #keras.metrics.CategoricalAccuracy(name='categorical_accuracy'),\n",
    "  keras.metrics.BinaryAccuracy(name='acc'),\n",
    "  keras.metrics.AUC(name='auc'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import save\n",
    "from numpy import load\n",
    "# X = np.array(X)\n",
    "# y = np.array(y)\n",
    "\n",
    "# Save data\n",
    "# save('X.npy', X)\n",
    "# save('y.npy', y)\n",
    "\n",
    "# Load data\n",
    "X = load('X.npy')\n",
    "y = load('y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthx = 256\n",
    "kern_init='he_uniform'\n",
    "bias_init='he_uniform'\n",
    "input_dims=41\n",
    "\n",
    "def identify_block(input_data):\n",
    "    x_shortcut = input_data\n",
    "    \n",
    "    #First Component\n",
    "    x = layers.Dense(lengthx, kernel_initializer=kern_init, bias_initializer=bias_init)(input_data)#, activation=activation)(input_data)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(tf.keras.activations.relu)(x)\n",
    "    \n",
    "    #Second Component\n",
    "    x = layers.Dense(lengthx, kernel_initializer=kern_init, bias_initializer=bias_init)(x)#, activation=activation)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(tf.keras.activations.relu)(x)\n",
    "    \n",
    "    #Third Component\n",
    "    x = layers.Dense(lengthx, kernel_initializer=kern_init, bias_initializer=bias_init)(x)#, activation=activation)(x)\n",
    "    x = layers.Activation(tf.keras.activations.relu)(x)\n",
    "    \n",
    "    #Final\n",
    "    x = layers.Add()([x, x_shortcut])\n",
    "    x = layers.Activation(tf.keras.activations.relu)(x)\n",
    "    return x\n",
    "\n",
    "def conv_block(input_data):\n",
    "    x_shortcut = input_data\n",
    "    \n",
    "    #First Component\n",
    "    x = layers.Dense(lengthx, kernel_initializer=kern_init, bias_initializer=bias_init)(input_data)#, activation=activation)(input_data)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(tf.keras.activations.relu)(x)\n",
    "    #Second Component\n",
    "    x = layers.Dense(lengthx, kernel_initializer=kern_init, bias_initializer=bias_init)(input_data)#, activation=activation)(input_data)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(tf.keras.activations.relu)(x)\n",
    "    #Third Component\n",
    "    x = layers.Dense(lengthx, kernel_initializer=kern_init, bias_initializer=bias_init)(x)#, activation=activation)(x)\n",
    "    x = layers.Activation(tf.keras.activations.relu)(x)\n",
    "    #Shortcut path\n",
    "    x_shortcut = layers.Dense(lengthx, kernel_initializer=kern_init, bias_initializer=bias_init)(x_shortcut)\n",
    "    x_shortcut = layers.BatchNormalization()(x_shortcut)\n",
    "    #Final\n",
    "    x = layers.Add()([x, x_shortcut])\n",
    "    x = layers.Activation(tf.keras.activations.relu)(x)\n",
    "    return x\n",
    "\n",
    "def KSmodel(optimizer='adam', kernel_initializer='he_uniform', dropout=0, activation='elu'):\n",
    "        \n",
    "    inputs = keras.Input(shape=input_dims)\n",
    "    x = layers.Dense(lengthx, kernel_initializer=kern_init, bias_initializer=bias_init)(inputs)#, activation=activation)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(tf.keras.activations.relu)(x)\n",
    "    \n",
    "    # Stage 2\n",
    "    x = conv_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "            \n",
    "    # Stage 3\n",
    "    x = conv_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    \n",
    "    # Stage 4\n",
    "    x = conv_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    \n",
    "    # Stage 5\n",
    "    x = conv_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    \n",
    "    # Stage 6\n",
    "    x = conv_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "        \n",
    "    # Stage 7\n",
    "    x = conv_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    \n",
    "    # Stage 8\n",
    "    x = conv_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    \n",
    "    # Stage 9\n",
    "    x = conv_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "    x = identify_block(x)\n",
    "        \n",
    "    outputs = layers.Dense(1, kernel_initializer=kern_init, bias_initializer=bias_init, activation='sigmoid')(x)\n",
    "       \n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=METRICS)\n",
    "\n",
    "    return model\n",
    "\n",
    "def SimpleModel(optimizer='adam', kernel_initializer='he_uniform', dropout=0, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Embedding(input_dim=input_dims, output_dim=64))\n",
    "    model.add(GRU(16, return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(GRU(16, return_sequences=True))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(GRU(16, return_sequences=True))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(GRU(16, return_sequences=True))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(GRU(16, return_sequences=True))\n",
    "    model.add(GRU(4, return_sequences=False))\n",
    "    \n",
    "    # model.add(Dense(1024, activation=activation))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dense(1024, activation=activation))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dense(32, activation=activation))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=METRICS)\n",
    "    return model\n",
    "\n",
    "clf = KerasClassifier(build_fn=KSmodel, verbose=1, epochs=2000, batch_size=5120, validation_split=0.1)#, callbacks=[my_callback_object])#, validation_split=0.1)#data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 186145 samples, validate on 20683 samples\n",
      "Epoch 1/2000\n",
      "186145/186145 [==============================] - 28s 151us/sample - loss: 0.7478 - precision: 0.6353 - recall: 0.8604 - acc: 0.6199 - auc: 0.5714 - val_loss: 382.2457 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.5000\n",
      "Epoch 2/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6509 - precision: 0.6423 - recall: 0.8939 - acc: 0.6377 - auc: 0.6168 - val_loss: 1.1257 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6538\n",
      "Epoch 3/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6474 - precision: 0.6425 - recall: 0.8977 - acc: 0.6390 - auc: 0.6278 - val_loss: 0.7535 - val_precision: 0.4371 - val_recall: 0.9974 - val_acc: 0.4431 - val_auc: 0.5787\n",
      "Epoch 4/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6465 - precision: 0.6450 - recall: 0.8891 - acc: 0.6399 - auc: 0.6300 - val_loss: 0.8132 - val_precision: 0.4350 - val_recall: 0.9990 - val_acc: 0.4380 - val_auc: 0.3931\n",
      "Epoch 5/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6459 - precision: 0.6442 - recall: 0.8970 - acc: 0.6409 - auc: 0.6338 - val_loss: 0.8251 - val_precision: 0.4342 - val_recall: 0.9998 - val_acc: 0.4363 - val_auc: 0.4095\n",
      "Epoch 6/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6465 - precision: 0.6452 - recall: 0.8915 - acc: 0.6408 - auc: 0.6341 - val_loss: 0.8281 - val_precision: 0.4336 - val_recall: 0.9998 - val_acc: 0.4348 - val_auc: 0.4642\n",
      "Epoch 7/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6433 - precision: 0.6452 - recall: 0.8918 - acc: 0.6408 - auc: 0.6349 - val_loss: 0.8450 - val_precision: 0.4340 - val_recall: 0.9998 - val_acc: 0.4358 - val_auc: 0.3954\n",
      "Epoch 8/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6412 - precision: 0.6460 - recall: 0.8913 - acc: 0.6417 - auc: 0.6367 - val_loss: 0.8121 - val_precision: 0.4340 - val_recall: 0.9998 - val_acc: 0.4357 - val_auc: 0.4406\n",
      "Epoch 9/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6445 - precision: 0.6461 - recall: 0.8903 - acc: 0.6416 - auc: 0.6395 - val_loss: 0.8120 - val_precision: 0.4340 - val_recall: 0.9998 - val_acc: 0.4358 - val_auc: 0.4728\n",
      "Epoch 10/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6446 - precision: 0.6467 - recall: 0.8905 - acc: 0.6424 - auc: 0.6395 - val_loss: 0.8167 - val_precision: 0.4337 - val_recall: 1.0000 - val_acc: 0.4351 - val_auc: 0.5206\n",
      "Epoch 11/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6407 - precision: 0.6473 - recall: 0.8874 - acc: 0.6423 - auc: 0.6410 - val_loss: 0.7545 - val_precision: 0.4341 - val_recall: 0.9996 - val_acc: 0.4361 - val_auc: 0.4778\n",
      "Epoch 12/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6421 - precision: 0.6475 - recall: 0.8871 - acc: 0.6425 - auc: 0.6444 - val_loss: 0.7308 - val_precision: 0.4355 - val_recall: 0.9984 - val_acc: 0.4394 - val_auc: 0.5192\n",
      "Epoch 13/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6394 - precision: 0.6479 - recall: 0.8842 - acc: 0.6422 - auc: 0.6443 - val_loss: 0.7103 - val_precision: 0.4488 - val_recall: 0.9620 - val_acc: 0.4724 - val_auc: 0.5672\n",
      "Epoch 14/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6377 - precision: 0.6495 - recall: 0.8800 - acc: 0.6431 - auc: 0.6487 - val_loss: 0.7078 - val_precision: 0.4496 - val_recall: 0.9606 - val_acc: 0.4741 - val_auc: 0.5413\n",
      "Epoch 15/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6395 - precision: 0.6482 - recall: 0.8849 - acc: 0.6427 - auc: 0.6486 - val_loss: 0.7024 - val_precision: 0.4580 - val_recall: 0.9502 - val_acc: 0.4920 - val_auc: 0.5539\n",
      "Epoch 16/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6341 - precision: 0.6553 - recall: 0.8540 - acc: 0.6428 - auc: 0.6595 - val_loss: 0.6886 - val_precision: 0.4817 - val_recall: 0.8757 - val_acc: 0.5386 - val_auc: 0.6568\n",
      "Epoch 17/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6390 - precision: 0.6545 - recall: 0.8629 - acc: 0.6445 - auc: 0.6545 - val_loss: 0.7382 - val_precision: 0.4666 - val_recall: 0.9311 - val_acc: 0.5096 - val_auc: 0.6458\n",
      "Epoch 18/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6378 - precision: 0.6519 - recall: 0.8686 - acc: 0.6429 - auc: 0.6524 - val_loss: 0.7747 - val_precision: 0.4355 - val_recall: 0.9985 - val_acc: 0.4393 - val_auc: 0.6123\n",
      "Epoch 19/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6459 - precision: 0.6460 - recall: 0.8872 - acc: 0.6406 - auc: 0.6339 - val_loss: 0.8117 - val_precision: 0.4342 - val_recall: 0.9998 - val_acc: 0.4363 - val_auc: 0.5859\n",
      "Epoch 20/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6440 - precision: 0.6466 - recall: 0.8895 - acc: 0.6420 - auc: 0.6357 - val_loss: 0.8313 - val_precision: 0.4340 - val_recall: 0.9998 - val_acc: 0.4357 - val_auc: 0.5216\n",
      "Epoch 21/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6446 - precision: 0.6467 - recall: 0.8882 - acc: 0.6418 - auc: 0.6406 - val_loss: 0.8179 - val_precision: 0.4340 - val_recall: 0.9998 - val_acc: 0.4357 - val_auc: 0.5634\n",
      "Epoch 22/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6435 - precision: 0.6459 - recall: 0.8912 - acc: 0.6416 - auc: 0.6410 - val_loss: 0.7884 - val_precision: 0.4335 - val_recall: 0.9998 - val_acc: 0.4345 - val_auc: 0.5872\n",
      "Epoch 23/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6413 - precision: 0.6470 - recall: 0.8877 - acc: 0.6421 - auc: 0.6497 - val_loss: 0.7635 - val_precision: 0.4339 - val_recall: 0.9998 - val_acc: 0.4355 - val_auc: 0.6101\n",
      "Epoch 24/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6439 - precision: 0.6476 - recall: 0.8882 - acc: 0.6430 - auc: 0.6434 - val_loss: 0.7364 - val_precision: 0.4340 - val_recall: 0.9998 - val_acc: 0.4357 - val_auc: 0.6552\n",
      "Epoch 25/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6365 - precision: 0.6514 - recall: 0.8726 - acc: 0.6434 - auc: 0.6522 - val_loss: 0.7388 - val_precision: 0.4341 - val_recall: 0.9998 - val_acc: 0.4361 - val_auc: 0.6463\n",
      "Epoch 26/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6351 - precision: 0.6541 - recall: 0.8710 - acc: 0.6462 - auc: 0.6584 - val_loss: 0.7383 - val_precision: 0.4389 - val_recall: 0.9959 - val_acc: 0.4474 - val_auc: 0.6578\n",
      "Epoch 27/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6299 - precision: 0.6663 - recall: 0.8474 - acc: 0.6538 - auc: 0.6677 - val_loss: 0.7200 - val_precision: 0.4503 - val_recall: 0.9812 - val_acc: 0.4736 - val_auc: 0.6514\n",
      "Epoch 28/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6321 - precision: 0.6712 - recall: 0.8482 - acc: 0.6596 - auc: 0.6719 - val_loss: 0.7200 - val_precision: 0.4563 - val_recall: 0.9604 - val_acc: 0.4877 - val_auc: 0.6526\n",
      "Epoch 29/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6361 - precision: 0.6690 - recall: 0.8328 - acc: 0.6525 - auc: 0.6666 - val_loss: 0.7736 - val_precision: 0.4357 - val_recall: 0.9985 - val_acc: 0.4397 - val_auc: 0.6502\n",
      "Epoch 30/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6434 - precision: 0.6482 - recall: 0.8816 - acc: 0.6419 - auc: 0.6452 - val_loss: 0.7393 - val_precision: 0.4343 - val_recall: 0.9997 - val_acc: 0.4363 - val_auc: 0.6478\n",
      "Epoch 31/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6383 - precision: 0.6480 - recall: 0.8897 - acc: 0.6438 - auc: 0.6492 - val_loss: 0.7374 - val_precision: 0.4357 - val_recall: 0.9985 - val_acc: 0.4398 - val_auc: 0.5984\n",
      "Epoch 32/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6362 - precision: 0.6521 - recall: 0.8672 - acc: 0.6427 - auc: 0.6565 - val_loss: 0.7106 - val_precision: 0.4343 - val_recall: 0.9996 - val_acc: 0.4366 - val_auc: 0.6419\n",
      "Epoch 33/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6394 - precision: 0.6507 - recall: 0.8762 - acc: 0.6435 - auc: 0.6502 - val_loss: 0.7230 - val_precision: 0.4437 - val_recall: 0.9898 - val_acc: 0.4587 - val_auc: 0.6458\n",
      "Epoch 34/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6389 - precision: 0.6601 - recall: 0.8488 - acc: 0.6471 - auc: 0.6582 - val_loss: 0.7000 - val_precision: 0.4500 - val_recall: 0.9813 - val_acc: 0.4730 - val_auc: 0.6503\n",
      "Epoch 35/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6339 - precision: 0.6593 - recall: 0.8502 - acc: 0.6465 - auc: 0.6586 - val_loss: 0.6868 - val_precision: 0.4792 - val_recall: 0.8971 - val_acc: 0.5336 - val_auc: 0.6584\n",
      "Epoch 36/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6323 - precision: 0.6687 - recall: 0.8259 - acc: 0.6501 - auc: 0.6656 - val_loss: 0.6851 - val_precision: 0.4813 - val_recall: 0.8871 - val_acc: 0.5375 - val_auc: 0.6407\n",
      "Epoch 37/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6333 - precision: 0.6671 - recall: 0.8463 - acc: 0.6544 - auc: 0.6704 - val_loss: 0.6769 - val_precision: 0.5133 - val_recall: 0.7753 - val_acc: 0.5847 - val_auc: 0.6675\n",
      "Epoch 38/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6304 - precision: 0.6731 - recall: 0.8379 - acc: 0.6586 - auc: 0.6736 - val_loss: 0.6806 - val_precision: 0.5064 - val_recall: 0.8034 - val_acc: 0.5761 - val_auc: 0.6348\n",
      "Epoch 39/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6274 - precision: 0.6739 - recall: 0.8307 - acc: 0.6573 - auc: 0.6726 - val_loss: 0.6775 - val_precision: 0.4916 - val_recall: 0.8494 - val_acc: 0.5548 - val_auc: 0.6755\n",
      "Epoch 40/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6307 - precision: 0.6723 - recall: 0.8440 - acc: 0.6595 - auc: 0.6753 - val_loss: 0.6690 - val_precision: 0.5007 - val_recall: 0.7808 - val_acc: 0.5682 - val_auc: 0.6484\n",
      "Epoch 41/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6287 - precision: 0.6742 - recall: 0.8384 - acc: 0.6599 - auc: 0.6754 - val_loss: 0.6737 - val_precision: 0.5065 - val_recall: 0.6666 - val_acc: 0.5748 - val_auc: 0.6310\n",
      "Epoch 42/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6255 - precision: 0.6706 - recall: 0.8560 - acc: 0.6614 - auc: 0.6771 - val_loss: 0.6670 - val_precision: 0.5839 - val_recall: 0.5182 - val_acc: 0.6317 - val_auc: 0.6651\n",
      "Epoch 43/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6241 - precision: 0.6740 - recall: 0.8498 - acc: 0.6632 - auc: 0.6777 - val_loss: 0.6797 - val_precision: 0.4847 - val_recall: 0.8647 - val_acc: 0.5438 - val_auc: 0.6644\n",
      "Epoch 44/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6246 - precision: 0.6693 - recall: 0.8667 - acc: 0.6630 - auc: 0.6780 - val_loss: 0.6665 - val_precision: 0.7425 - val_recall: 0.0825 - val_acc: 0.5906 - val_auc: 0.6521\n",
      "Epoch 45/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6229 - precision: 0.6711 - recall: 0.8639 - acc: 0.6643 - auc: 0.6799 - val_loss: 0.6642 - val_precision: 0.6697 - val_recall: 0.3012 - val_acc: 0.6334 - val_auc: 0.6746\n",
      "Epoch 46/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6233 - precision: 0.6719 - recall: 0.8641 - acc: 0.6652 - auc: 0.6814 - val_loss: 0.6627 - val_precision: 0.5827 - val_recall: 0.5848 - val_acc: 0.6391 - val_auc: 0.6851\n",
      "Epoch 47/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6200 - precision: 0.6723 - recall: 0.8622 - acc: 0.6651 - auc: 0.6831 - val_loss: 0.6710 - val_precision: 0.4877 - val_recall: 0.8826 - val_acc: 0.5481 - val_auc: 0.6852\n",
      "Epoch 48/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6209 - precision: 0.6755 - recall: 0.8574 - acc: 0.6673 - auc: 0.6860 - val_loss: 0.6684 - val_precision: 0.5824 - val_recall: 0.4235 - val_acc: 0.6192 - val_auc: 0.6568\n",
      "Epoch 49/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6204 - precision: 0.6751 - recall: 0.8610 - acc: 0.6680 - auc: 0.6860 - val_loss: 0.6584 - val_precision: 0.6215 - val_recall: 0.4506 - val_acc: 0.6435 - val_auc: 0.6826\n",
      "Epoch 50/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6207 - precision: 0.6769 - recall: 0.8509 - acc: 0.6668 - auc: 0.6872 - val_loss: 0.6962 - val_precision: 0.4577 - val_recall: 0.9587 - val_acc: 0.4906 - val_auc: 0.6564\n",
      "Epoch 51/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6352 - precision: 0.6464 - recall: 0.9011 - acc: 0.6449 - auc: 0.6533 - val_loss: 0.7060 - val_precision: 0.4447 - val_recall: 0.9885 - val_acc: 0.4609 - val_auc: 0.4890\n",
      "Epoch 52/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6401 - precision: 0.6472 - recall: 0.8894 - acc: 0.6428 - auc: 0.6399 - val_loss: 0.6909 - val_precision: 0.4563 - val_recall: 0.9604 - val_acc: 0.4877 - val_auc: 0.6413\n",
      "Epoch 53/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6391 - precision: 0.6491 - recall: 0.8855 - acc: 0.6441 - auc: 0.6444 - val_loss: 0.6858 - val_precision: 0.4657 - val_recall: 0.9390 - val_acc: 0.5076 - val_auc: 0.6341\n",
      "Epoch 54/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6406 - precision: 0.6479 - recall: 0.8841 - acc: 0.6422 - auc: 0.6452 - val_loss: 0.7056 - val_precision: 0.4436 - val_recall: 0.9898 - val_acc: 0.4584 - val_auc: 0.5232\n",
      "Epoch 55/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6360 - precision: 0.6506 - recall: 0.8709 - acc: 0.6419 - auc: 0.6506 - val_loss: 0.7027 - val_precision: 0.4438 - val_recall: 0.9893 - val_acc: 0.4589 - val_auc: 0.6477\n",
      "Epoch 56/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6445 - precision: 0.6473 - recall: 0.8833 - acc: 0.6413 - auc: 0.6409 - val_loss: 0.6961 - val_precision: 0.4482 - val_recall: 0.9848 - val_acc: 0.4689 - val_auc: 0.6593\n",
      "Epoch 57/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6415 - precision: 0.6467 - recall: 0.8922 - acc: 0.6429 - auc: 0.6440 - val_loss: 0.6923 - val_precision: 0.4611 - val_recall: 0.9487 - val_acc: 0.4980 - val_auc: 0.6607\n",
      "Epoch 58/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6345 - precision: 0.6488 - recall: 0.8838 - acc: 0.6433 - auc: 0.6543 - val_loss: 0.6981 - val_precision: 0.4492 - val_recall: 0.9831 - val_acc: 0.4712 - val_auc: 0.6554\n",
      "Epoch 59/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6387 - precision: 0.6473 - recall: 0.8850 - acc: 0.6417 - auc: 0.6438 - val_loss: 0.7173 - val_precision: 0.4380 - val_recall: 0.9965 - val_acc: 0.4452 - val_auc: 0.5750\n",
      "Epoch 60/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6353 - precision: 0.6500 - recall: 0.8760 - acc: 0.6426 - auc: 0.6510 - val_loss: 0.6969 - val_precision: 0.4773 - val_recall: 0.8540 - val_acc: 0.5322 - val_auc: 0.5118\n",
      "Epoch 61/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6375 - precision: 0.6544 - recall: 0.8595 - acc: 0.6433 - auc: 0.6522 - val_loss: 0.7027 - val_precision: 0.4621 - val_recall: 0.9465 - val_acc: 0.5001 - val_auc: 0.6072\n",
      "Epoch 62/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6386 - precision: 0.6491 - recall: 0.8827 - acc: 0.6433 - auc: 0.6465 - val_loss: 0.7065 - val_precision: 0.4383 - val_recall: 0.9963 - val_acc: 0.4459 - val_auc: 0.6156\n",
      "Epoch 63/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6383 - precision: 0.6541 - recall: 0.8613 - acc: 0.6436 - auc: 0.6482 - val_loss: 0.7245 - val_precision: 0.4359 - val_recall: 0.9982 - val_acc: 0.4404 - val_auc: 0.6204\n",
      "Epoch 64/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6400 - precision: 0.6478 - recall: 0.8874 - acc: 0.6430 - auc: 0.6472 - val_loss: 0.7208 - val_precision: 0.4355 - val_recall: 0.9985 - val_acc: 0.4394 - val_auc: 0.6260\n",
      "Epoch 65/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6329 - precision: 0.6618 - recall: 0.8473 - acc: 0.6485 - auc: 0.6646 - val_loss: 0.7030 - val_precision: 0.4663 - val_recall: 0.9385 - val_acc: 0.5086 - val_auc: 0.5962\n",
      "Epoch 66/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6309 - precision: 0.6694 - recall: 0.8365 - acc: 0.6540 - auc: 0.6688 - val_loss: 0.6913 - val_precision: 0.4770 - val_recall: 0.9004 - val_acc: 0.5297 - val_auc: 0.5326\n",
      "Epoch 67/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6282 - precision: 0.6715 - recall: 0.8445 - acc: 0.6588 - auc: 0.6747 - val_loss: 0.6931 - val_precision: 0.4637 - val_recall: 0.9442 - val_acc: 0.5033 - val_auc: 0.5923\n",
      "Epoch 68/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6283 - precision: 0.6648 - recall: 0.8637 - acc: 0.6569 - auc: 0.6697 - val_loss: 0.6997 - val_precision: 0.4584 - val_recall: 0.9552 - val_acc: 0.4923 - val_auc: 0.6642\n",
      "Epoch 69/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6334 - precision: 0.6570 - recall: 0.8670 - acc: 0.6486 - auc: 0.6595 - val_loss: 0.6994 - val_precision: 0.4492 - val_recall: 0.9831 - val_acc: 0.4712 - val_auc: 0.6706\n",
      "Epoch 70/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6277 - precision: 0.6684 - recall: 0.8530 - acc: 0.6579 - auc: 0.6715 - val_loss: 0.7009 - val_precision: 0.4672 - val_recall: 0.9372 - val_acc: 0.5103 - val_auc: 0.6628\n",
      "Epoch 71/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6327 - precision: 0.6680 - recall: 0.8381 - acc: 0.6529 - auc: 0.6675 - val_loss: 0.6879 - val_precision: 0.4547 - val_recall: 0.9627 - val_acc: 0.4843 - val_auc: 0.6693\n",
      "Epoch 72/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6331 - precision: 0.6725 - recall: 0.8360 - acc: 0.6574 - auc: 0.6717 - val_loss: 0.6832 - val_precision: 0.4859 - val_recall: 0.8738 - val_acc: 0.5454 - val_auc: 0.6788\n",
      "Epoch 73/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6385 - precision: 0.6479 - recall: 0.8956 - acc: 0.6454 - auc: 0.6491 - val_loss: 0.6964 - val_precision: 0.4563 - val_recall: 0.9599 - val_acc: 0.4878 - val_auc: 0.6611\n",
      "Epoch 74/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6365 - precision: 0.6501 - recall: 0.8864 - acc: 0.6457 - auc: 0.6472 - val_loss: 0.6905 - val_precision: 0.4564 - val_recall: 0.9603 - val_acc: 0.4879 - val_auc: 0.6580\n",
      "Epoch 75/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6354 - precision: 0.6494 - recall: 0.8909 - acc: 0.6459 - auc: 0.6517 - val_loss: 0.7064 - val_precision: 0.4556 - val_recall: 0.9618 - val_acc: 0.4861 - val_auc: 0.6343\n",
      "Epoch 76/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6360 - precision: 0.6492 - recall: 0.8836 - acc: 0.6438 - auc: 0.6487 - val_loss: 0.7106 - val_precision: 0.4495 - val_recall: 0.9822 - val_acc: 0.4717 - val_auc: 0.6439\n",
      "Epoch 77/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6342 - precision: 0.6496 - recall: 0.8912 - acc: 0.6463 - auc: 0.6537 - val_loss: 0.7005 - val_precision: 0.4500 - val_recall: 0.9813 - val_acc: 0.4730 - val_auc: 0.5973\n",
      "Epoch 78/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6270 - precision: 0.6608 - recall: 0.8657 - acc: 0.6528 - auc: 0.6708 - val_loss: 0.6884 - val_precision: 0.4704 - val_recall: 0.9360 - val_acc: 0.5163 - val_auc: 0.5751\n",
      "Epoch 79/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6314 - precision: 0.6659 - recall: 0.8563 - acc: 0.6560 - auc: 0.6679 - val_loss: 0.6924 - val_precision: 0.4645 - val_recall: 0.9426 - val_acc: 0.5050 - val_auc: 0.6781\n",
      "Epoch 80/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6286 - precision: 0.6678 - recall: 0.8524 - acc: 0.6571 - auc: 0.6723 - val_loss: 0.7220 - val_precision: 0.4565 - val_recall: 0.9599 - val_acc: 0.4882 - val_auc: 0.6256\n",
      "Epoch 81/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6275 - precision: 0.6694 - recall: 0.8524 - acc: 0.6588 - auc: 0.6729 - val_loss: 0.7247 - val_precision: 0.4565 - val_recall: 0.9599 - val_acc: 0.4881 - val_auc: 0.6012\n",
      "Epoch 82/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6227 - precision: 0.6719 - recall: 0.8597 - acc: 0.6640 - auc: 0.6779 - val_loss: 0.7037 - val_precision: 0.4571 - val_recall: 0.9594 - val_acc: 0.4893 - val_auc: 0.5040\n",
      "Epoch 83/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6211 - precision: 0.6693 - recall: 0.8700 - acc: 0.6641 - auc: 0.6789 - val_loss: 0.6963 - val_precision: 0.3402 - val_recall: 0.2301 - val_acc: 0.4738 - val_auc: 0.4799\n",
      "Epoch 84/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6208 - precision: 0.6710 - recall: 0.8657 - acc: 0.6647 - auc: 0.6804 - val_loss: 0.6830 - val_precision: 0.3296 - val_recall: 0.1457 - val_acc: 0.5022 - val_auc: 0.4987\n",
      "Epoch 85/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6200 - precision: 0.6705 - recall: 0.8690 - acc: 0.6652 - auc: 0.6811 - val_loss: 0.6860 - val_precision: 0.3331 - val_recall: 0.0607 - val_acc: 0.5410 - val_auc: 0.5150\n",
      "Epoch 86/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6205 - precision: 0.6713 - recall: 0.8689 - acc: 0.6661 - auc: 0.6829 - val_loss: 0.6840 - val_precision: 0.5667 - val_recall: 0.0019 - val_acc: 0.5675 - val_auc: 0.5632\n",
      "Epoch 87/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6185 - precision: 0.6727 - recall: 0.8633 - acc: 0.6660 - auc: 0.6856 - val_loss: 0.6685 - val_precision: 0.4166 - val_recall: 0.0784 - val_acc: 0.5537 - val_auc: 0.6217\n",
      "Epoch 88/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6214 - precision: 0.6706 - recall: 0.8588 - acc: 0.6622 - auc: 0.6795 - val_loss: 0.6765 - val_precision: 0.5311 - val_recall: 0.4138 - val_acc: 0.5883 - val_auc: 0.5986\n",
      "Epoch 89/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6264 - precision: 0.6627 - recall: 0.8761 - acc: 0.6581 - auc: 0.6704 - val_loss: 0.7193 - val_precision: 0.2988 - val_recall: 0.1120 - val_acc: 0.5021 - val_auc: 0.4237\n",
      "Epoch 90/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6191 - precision: 0.6716 - recall: 0.8675 - acc: 0.6659 - auc: 0.6851 - val_loss: 0.6967 - val_precision: 0.2830 - val_recall: 0.0806 - val_acc: 0.5139 - val_auc: 0.4686\n",
      "Epoch 91/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6176 - precision: 0.6775 - recall: 0.8542 - acc: 0.6686 - auc: 0.6887 - val_loss: 0.6862 - val_precision: 0.2839 - val_recall: 0.0594 - val_acc: 0.5282 - val_auc: 0.5303\n",
      "Epoch 92/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6310 - precision: 0.6589 - recall: 0.8651 - acc: 0.6504 - auc: 0.6597 - val_loss: 0.6748 - val_precision: 0.5141 - val_recall: 0.7716 - val_acc: 0.5857 - val_auc: 0.6647\n",
      "Epoch 93/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6406 - precision: 0.6487 - recall: 0.8811 - acc: 0.6423 - auc: 0.6397 - val_loss: 1.2926 - val_precision: 0.4366 - val_recall: 0.6983 - val_acc: 0.4796 - val_auc: 0.4855\n",
      "Epoch 94/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6397 - precision: 0.6485 - recall: 0.8870 - acc: 0.6437 - auc: 0.6458 - val_loss: 0.7124 - val_precision: 0.4415 - val_recall: 0.9931 - val_acc: 0.4535 - val_auc: 0.6340\n",
      "Epoch 95/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6364 - precision: 0.6484 - recall: 0.8858 - acc: 0.6433 - auc: 0.6489 - val_loss: 0.7184 - val_precision: 0.4413 - val_recall: 0.9931 - val_acc: 0.4529 - val_auc: 0.6450\n",
      "Epoch 96/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6368 - precision: 0.6508 - recall: 0.8794 - acc: 0.6445 - auc: 0.6472 - val_loss: 0.7091 - val_precision: 0.4475 - val_recall: 0.9858 - val_acc: 0.4673 - val_auc: 0.5737\n",
      "Epoch 97/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6364 - precision: 0.6501 - recall: 0.8831 - acc: 0.6448 - auc: 0.6484 - val_loss: 0.6984 - val_precision: 0.4576 - val_recall: 0.9590 - val_acc: 0.4904 - val_auc: 0.6617\n",
      "Epoch 98/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6352 - precision: 0.6509 - recall: 0.8745 - acc: 0.6432 - auc: 0.6546 - val_loss: 0.7057 - val_precision: 0.4497 - val_recall: 0.9819 - val_acc: 0.4724 - val_auc: 0.6582\n",
      "Epoch 99/2000\n",
      "186145/186145 [==============================] - 6s 35us/sample - loss: 0.6375 - precision: 0.6472 - recall: 0.8876 - acc: 0.6423 - auc: 0.6458 - val_loss: 0.6894 - val_precision: 0.4419 - val_recall: 0.9930 - val_acc: 0.4543 - val_auc: 0.6640\n",
      "Epoch 100/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6365 - precision: 0.6469 - recall: 0.8908 - acc: 0.6428 - auc: 0.6472 - val_loss: 0.6954 - val_precision: 0.4420 - val_recall: 0.9930 - val_acc: 0.4545 - val_auc: 0.5554\n",
      "Epoch 101/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6313 - precision: 0.6575 - recall: 0.8673 - acc: 0.6493 - auc: 0.6608 - val_loss: 0.6843 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.5370\n",
      "Epoch 102/2000\n",
      "186145/186145 [==============================] - 7s 35us/sample - loss: 0.6335 - precision: 0.6513 - recall: 0.8773 - acc: 0.6445 - auc: 0.6550 - val_loss: 0.6946 - val_precision: 0.4624 - val_recall: 0.9459 - val_acc: 0.5008 - val_auc: 0.6470\n",
      "Epoch 103/2000\n",
      "186145/186145 [==============================] - 8s 43us/sample - loss: 0.6388 - precision: 0.6492 - recall: 0.8879 - acc: 0.6449 - auc: 0.6524 - val_loss: 0.6897 - val_precision: 0.4602 - val_recall: 0.9498 - val_acc: 0.4962 - val_auc: 0.5650\n",
      "Epoch 104/2000\n",
      "186145/186145 [==============================] - 9s 46us/sample - loss: 0.6319 - precision: 0.6496 - recall: 0.8821 - acc: 0.6438 - auc: 0.6593 - val_loss: 0.6948 - val_precision: 0.4480 - val_recall: 0.9848 - val_acc: 0.4683 - val_auc: 0.4686\n",
      "Epoch 105/2000\n",
      "186145/186145 [==============================] - 9s 47us/sample - loss: 0.6326 - precision: 0.6534 - recall: 0.8677 - acc: 0.6445 - auc: 0.6585 - val_loss: 0.6943 - val_precision: 0.4479 - val_recall: 0.9848 - val_acc: 0.4681 - val_auc: 0.6713\n",
      "Epoch 106/2000\n",
      "186145/186145 [==============================] - 7s 36us/sample - loss: 0.6330 - precision: 0.6590 - recall: 0.8488 - acc: 0.6457 - auc: 0.6596 - val_loss: 0.6861 - val_precision: 0.4709 - val_recall: 0.9359 - val_acc: 0.5172 - val_auc: 0.6712\n",
      "Epoch 107/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6334 - precision: 0.6523 - recall: 0.8760 - acc: 0.6454 - auc: 0.6567 - val_loss: 0.6906 - val_precision: 0.4476 - val_recall: 0.9855 - val_acc: 0.4675 - val_auc: 0.6495\n",
      "Epoch 108/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6298 - precision: 0.6659 - recall: 0.8371 - acc: 0.6502 - auc: 0.6696 - val_loss: 0.6795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6211\n",
      "Epoch 109/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6330 - precision: 0.6562 - recall: 0.8588 - acc: 0.6453 - auc: 0.6586 - val_loss: 0.6877 - val_precision: 0.4636 - val_recall: 0.9460 - val_acc: 0.5031 - val_auc: 0.6266\n",
      "Epoch 110/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6299 - precision: 0.6543 - recall: 0.8819 - acc: 0.6496 - auc: 0.6657 - val_loss: 0.6704 - val_precision: 0.5955 - val_recall: 0.5140 - val_acc: 0.6387 - val_auc: 0.6765\n",
      "Epoch 111/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6284 - precision: 0.6659 - recall: 0.8545 - acc: 0.6555 - auc: 0.6714 - val_loss: 0.6718 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6774\n",
      "Epoch 112/2000\n",
      "186145/186145 [==============================] - 7s 35us/sample - loss: 0.6233 - precision: 0.6700 - recall: 0.8552 - acc: 0.6604 - auc: 0.6767 - val_loss: 0.6660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6573\n",
      "Epoch 113/2000\n",
      "186145/186145 [==============================] - 7s 38us/sample - loss: 0.6267 - precision: 0.6623 - recall: 0.8645 - acc: 0.6542 - auc: 0.6725 - val_loss: 0.6654 - val_precision: 0.5395 - val_recall: 0.7022 - val_acc: 0.6119 - val_auc: 0.6777\n",
      "Epoch 114/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6219 - precision: 0.6727 - recall: 0.8492 - acc: 0.6616 - auc: 0.6793 - val_loss: 0.6555 - val_precision: 0.5364 - val_recall: 0.7113 - val_acc: 0.6091 - val_auc: 0.6785\n",
      "Epoch 115/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6218 - precision: 0.6703 - recall: 0.8611 - acc: 0.6626 - auc: 0.6797 - val_loss: 0.6743 - val_precision: 0.4983 - val_recall: 0.8440 - val_acc: 0.5649 - val_auc: 0.6835\n",
      "Epoch 116/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6260 - precision: 0.6699 - recall: 0.8555 - acc: 0.6603 - auc: 0.6757 - val_loss: 0.7729 - val_precision: 0.4477 - val_recall: 0.9852 - val_acc: 0.4677 - val_auc: 0.6469\n",
      "Epoch 117/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6276 - precision: 0.6650 - recall: 0.8604 - acc: 0.6562 - auc: 0.6719 - val_loss: 0.7263 - val_precision: 0.4415 - val_recall: 0.9932 - val_acc: 0.4535 - val_auc: 0.4601\n",
      "Epoch 118/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6227 - precision: 0.6705 - recall: 0.8582 - acc: 0.6619 - auc: 0.6790 - val_loss: 0.7056 - val_precision: 0.4438 - val_recall: 0.9896 - val_acc: 0.4588 - val_auc: 0.4867\n",
      "Epoch 119/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6214 - precision: 0.6722 - recall: 0.8543 - acc: 0.6626 - auc: 0.6804 - val_loss: 0.7249 - val_precision: 0.4475 - val_recall: 0.9856 - val_acc: 0.4673 - val_auc: 0.6867\n",
      "Epoch 120/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6223 - precision: 0.6749 - recall: 0.8456 - acc: 0.6630 - auc: 0.6816 - val_loss: 1.1308 - val_precision: 0.4544 - val_recall: 0.9632 - val_acc: 0.4837 - val_auc: 0.5862\n",
      "Epoch 121/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6250 - precision: 0.6721 - recall: 0.8360 - acc: 0.6569 - auc: 0.6768 - val_loss: 0.7422 - val_precision: 0.2068 - val_recall: 0.0622 - val_acc: 0.4909 - val_auc: 0.3299\n",
      "Epoch 122/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6239 - precision: 0.6692 - recall: 0.8513 - acc: 0.6582 - auc: 0.6787 - val_loss: 0.8460 - val_precision: 0.4339 - val_recall: 0.9998 - val_acc: 0.4355 - val_auc: 0.6733\n",
      "Epoch 123/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6267 - precision: 0.6607 - recall: 0.8678 - acc: 0.6532 - auc: 0.6710 - val_loss: 0.7147 - val_precision: 0.4355 - val_recall: 0.9985 - val_acc: 0.4394 - val_auc: 0.6721\n",
      "Epoch 124/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6269 - precision: 0.6603 - recall: 0.8720 - acc: 0.6540 - auc: 0.6725 - val_loss: 0.6961 - val_precision: 0.4497 - val_recall: 0.9822 - val_acc: 0.4723 - val_auc: 0.5501\n",
      "Epoch 125/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6210 - precision: 0.6738 - recall: 0.8517 - acc: 0.6636 - auc: 0.6825 - val_loss: 0.6813 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.5064\n",
      "Epoch 126/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6233 - precision: 0.6709 - recall: 0.8526 - acc: 0.6607 - auc: 0.6803 - val_loss: 0.6977 - val_precision: 0.4415 - val_recall: 0.9932 - val_acc: 0.4535 - val_auc: 0.4952\n",
      "Epoch 127/2000\n",
      "186145/186145 [==============================] - 7s 38us/sample - loss: 0.6180 - precision: 0.6719 - recall: 0.8624 - acc: 0.6648 - auc: 0.6872 - val_loss: 0.6946 - val_precision: 0.4590 - val_recall: 0.9527 - val_acc: 0.4936 - val_auc: 0.6142\n",
      "Epoch 128/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6244 - precision: 0.6700 - recall: 0.8523 - acc: 0.6595 - auc: 0.6823 - val_loss: 0.9623 - val_precision: 0.4562 - val_recall: 0.9607 - val_acc: 0.4876 - val_auc: 0.6185\n",
      "Epoch 129/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6308 - precision: 0.6553 - recall: 0.8754 - acc: 0.6490 - auc: 0.6624 - val_loss: 0.7066 - val_precision: 0.4414 - val_recall: 0.9933 - val_acc: 0.4533 - val_auc: 0.5263\n",
      "Epoch 130/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6248 - precision: 0.6640 - recall: 0.8577 - acc: 0.6542 - auc: 0.6745 - val_loss: 0.7264 - val_precision: 0.4350 - val_recall: 0.9990 - val_acc: 0.4381 - val_auc: 0.6725\n",
      "Epoch 131/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6332 - precision: 0.6505 - recall: 0.8838 - acc: 0.6453 - auc: 0.6535 - val_loss: 0.7270 - val_precision: 0.4350 - val_recall: 0.9990 - val_acc: 0.4381 - val_auc: 0.6219\n",
      "Epoch 132/2000\n",
      "186145/186145 [==============================] - 7s 38us/sample - loss: 0.6268 - precision: 0.6562 - recall: 0.8843 - acc: 0.6526 - auc: 0.6707 - val_loss: 0.7075 - val_precision: 0.4348 - val_recall: 0.9990 - val_acc: 0.4378 - val_auc: 0.4590\n",
      "Epoch 133/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6128 - precision: 0.6838 - recall: 0.8371 - acc: 0.6700 - auc: 0.6972 - val_loss: 0.6932 - val_precision: 0.4430 - val_recall: 0.9912 - val_acc: 0.4569 - val_auc: 0.6773\n",
      "Epoch 134/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6330 - precision: 0.6524 - recall: 0.8756 - acc: 0.6455 - auc: 0.6551 - val_loss: 0.7080 - val_precision: 0.4372 - val_recall: 0.9974 - val_acc: 0.4434 - val_auc: 0.6575\n",
      "Epoch 135/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6333 - precision: 0.6497 - recall: 0.8879 - acc: 0.6456 - auc: 0.6535 - val_loss: 0.7263 - val_precision: 0.4357 - val_recall: 0.9985 - val_acc: 0.4398 - val_auc: 0.3920\n",
      "Epoch 136/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6285 - precision: 0.6551 - recall: 0.8859 - acc: 0.6517 - auc: 0.6653 - val_loss: 0.7046 - val_precision: 0.4396 - val_recall: 0.9954 - val_acc: 0.4489 - val_auc: 0.4016\n",
      "Epoch 137/2000\n",
      "186145/186145 [==============================] - 7s 38us/sample - loss: 0.6242 - precision: 0.6641 - recall: 0.8621 - acc: 0.6556 - auc: 0.6786 - val_loss: 0.6879 - val_precision: 0.4512 - val_recall: 0.9807 - val_acc: 0.4755 - val_auc: 0.5723\n",
      "Epoch 138/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6224 - precision: 0.6717 - recall: 0.8398 - acc: 0.6575 - auc: 0.6814 - val_loss: 0.7565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.4538\n",
      "Epoch 139/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6226 - precision: 0.6636 - recall: 0.8637 - acc: 0.6555 - auc: 0.6790 - val_loss: 0.7007 - val_precision: 0.4497 - val_recall: 0.9821 - val_acc: 0.4722 - val_auc: 0.6189\n",
      "Epoch 140/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6284 - precision: 0.6602 - recall: 0.8504 - acc: 0.6476 - auc: 0.6684 - val_loss: 0.9078 - val_precision: 0.4590 - val_recall: 0.9545 - val_acc: 0.4936 - val_auc: 0.6687\n",
      "Epoch 141/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6213 - precision: 0.6709 - recall: 0.8551 - acc: 0.6613 - auc: 0.6826 - val_loss: 0.8128 - val_precision: 0.4671 - val_recall: 0.9378 - val_acc: 0.5102 - val_auc: 0.6829\n",
      "Epoch 142/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6270 - precision: 0.6657 - recall: 0.8604 - acc: 0.6570 - auc: 0.6705 - val_loss: 0.7567 - val_precision: 0.4392 - val_recall: 0.9956 - val_acc: 0.4481 - val_auc: 0.6614\n",
      "Epoch 143/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6371 - precision: 0.6496 - recall: 0.8816 - acc: 0.6436 - auc: 0.6429 - val_loss: 0.7184 - val_precision: 0.4502 - val_recall: 0.9813 - val_acc: 0.4733 - val_auc: 0.6623\n",
      "Epoch 144/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6372 - precision: 0.6492 - recall: 0.8834 - acc: 0.6436 - auc: 0.6426 - val_loss: 0.7045 - val_precision: 0.4500 - val_recall: 0.9813 - val_acc: 0.4730 - val_auc: 0.6515\n",
      "Epoch 145/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6362 - precision: 0.6491 - recall: 0.8854 - acc: 0.6440 - auc: 0.6464 - val_loss: 0.6851 - val_precision: 0.4792 - val_recall: 0.8971 - val_acc: 0.5336 - val_auc: 0.6583\n",
      "Epoch 146/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6343 - precision: 0.6485 - recall: 0.8901 - acc: 0.6446 - auc: 0.6497 - val_loss: 0.6875 - val_precision: 0.4676 - val_recall: 0.9371 - val_acc: 0.5111 - val_auc: 0.6438\n",
      "Epoch 147/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6358 - precision: 0.6498 - recall: 0.8821 - acc: 0.6440 - auc: 0.6455 - val_loss: 0.6817 - val_precision: 0.4928 - val_recall: 0.8448 - val_acc: 0.5566 - val_auc: 0.6610\n",
      "Epoch 148/2000\n",
      "186145/186145 [==============================] - 7s 38us/sample - loss: 0.6347 - precision: 0.6487 - recall: 0.8885 - acc: 0.6444 - auc: 0.6487 - val_loss: 0.6717 - val_precision: 0.5519 - val_recall: 0.6359 - val_acc: 0.6191 - val_auc: 0.6602c: 0.6444 - auc: 0.64\n",
      "Epoch 149/2000\n",
      "186145/186145 [==============================] - 7s 38us/sample - loss: 0.6380 - precision: 0.6496 - recall: 0.8849 - acc: 0.6446 - auc: 0.6461 - val_loss: 0.7499 - val_precision: 0.4380 - val_recall: 0.9965 - val_acc: 0.4452 - val_auc: 0.3969\n",
      "Epoch 150/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6355 - precision: 0.6535 - recall: 0.8754 - acc: 0.6467 - auc: 0.6572 - val_loss: 0.7882 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.5573\n",
      "Epoch 151/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6334 - precision: 0.6524 - recall: 0.8749 - acc: 0.6453 - auc: 0.6551 - val_loss: 0.6644 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6524\n",
      "Epoch 152/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6335 - precision: 0.6507 - recall: 0.8816 - acc: 0.6450 - auc: 0.6543 - val_loss: 0.6730 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6228\n",
      "Epoch 153/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6339 - precision: 0.6555 - recall: 0.8635 - acc: 0.6458 - auc: 0.6558 - val_loss: 0.6821 - val_precision: 0.4925 - val_recall: 0.8459 - val_acc: 0.5563 - val_auc: 0.6473\n",
      "Epoch 154/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6320 - precision: 0.6507 - recall: 0.8830 - acc: 0.6454 - auc: 0.6576 - val_loss: 0.6787 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6104\n",
      "Epoch 155/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6249 - precision: 0.6718 - recall: 0.8395 - acc: 0.6576 - auc: 0.6757 - val_loss: 0.6819 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6154\n",
      "Epoch 156/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6205 - precision: 0.6751 - recall: 0.8476 - acc: 0.6638 - auc: 0.6828 - val_loss: 0.6853 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6605\n",
      "Epoch 157/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6350 - precision: 0.6515 - recall: 0.8759 - acc: 0.6444 - auc: 0.6509 - val_loss: 0.6685 - val_precision: 0.5037 - val_recall: 0.8143 - val_acc: 0.5725 - val_auc: 0.6679\n",
      "Epoch 158/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6341 - precision: 0.6501 - recall: 0.8817 - acc: 0.6443 - auc: 0.6499 - val_loss: 0.6720 - val_precision: 0.5060 - val_recall: 0.8118 - val_acc: 0.5756 - val_auc: 0.6626\n",
      "Epoch 159/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6346 - precision: 0.6506 - recall: 0.8833 - acc: 0.6454 - auc: 0.6541 - val_loss: 0.6715 - val_precision: 0.5135 - val_recall: 0.7756 - val_acc: 0.5849 - val_auc: 0.6425\n",
      "Epoch 160/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6301 - precision: 0.6525 - recall: 0.8753 - acc: 0.6454 - auc: 0.6622 - val_loss: 0.6776 - val_precision: 0.5528 - val_recall: 0.6423 - val_acc: 0.6204 - val_auc: 0.6705\n",
      "Epoch 161/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6279 - precision: 0.6552 - recall: 0.8785 - acc: 0.6497 - auc: 0.6683 - val_loss: 0.6752 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6265\n",
      "Epoch 162/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6254 - precision: 0.6697 - recall: 0.8481 - acc: 0.6579 - auc: 0.6746 - val_loss: 0.6838 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5672 - val_auc: 0.6770\n",
      "Epoch 163/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6240 - precision: 0.6709 - recall: 0.8455 - acc: 0.6585 - auc: 0.6784 - val_loss: 0.6780 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6637\n",
      "Epoch 164/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6268 - precision: 0.6657 - recall: 0.8453 - acc: 0.6525 - auc: 0.6717 - val_loss: 0.6712 - val_precision: 0.5588 - val_recall: 0.6317 - val_acc: 0.6248 - val_auc: 0.6707\n",
      "Epoch 165/2000\n",
      "186145/186145 [==============================] - 7s 38us/sample - loss: 0.6265 - precision: 0.6631 - recall: 0.8551 - acc: 0.6524 - auc: 0.6717 - val_loss: 0.6813 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.5559\n",
      "Epoch 166/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6259 - precision: 0.6616 - recall: 0.8591 - acc: 0.6519 - auc: 0.6727 - val_loss: 0.6791 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.5836\n",
      "Epoch 167/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6210 - precision: 0.6740 - recall: 0.8503 - acc: 0.6634 - auc: 0.6846 - val_loss: 1.0468 - val_precision: 0.4612 - val_recall: 0.9486 - val_acc: 0.4982 - val_auc: 0.4993\n",
      "Epoch 168/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6243 - precision: 0.6632 - recall: 0.8685 - acc: 0.6564 - auc: 0.6762 - val_loss: 0.6926 - val_precision: 0.4661 - val_recall: 0.9387 - val_acc: 0.5083 - val_auc: 0.6765\n",
      "Epoch 169/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6318 - precision: 0.6514 - recall: 0.8834 - acc: 0.6464 - auc: 0.6615 - val_loss: 0.7665 - val_precision: 0.4656 - val_recall: 0.9390 - val_acc: 0.5072 - val_auc: 0.6788\n",
      "Epoch 170/2000\n",
      "186145/186145 [==============================] - 7s 38us/sample - loss: 0.6243 - precision: 0.6676 - recall: 0.8490 - acc: 0.6558 - auc: 0.6766 - val_loss: 0.7641 - val_precision: 0.4284 - val_recall: 0.6493 - val_acc: 0.4734 - val_auc: 0.4779\n",
      "Epoch 171/2000\n",
      "186145/186145 [==============================] - 7s 38us/sample - loss: 0.6183 - precision: 0.6789 - recall: 0.8391 - acc: 0.6653 - auc: 0.6894 - val_loss: 1.0857 - val_precision: 0.4634 - val_recall: 0.9447 - val_acc: 0.5028 - val_auc: 0.4966\n",
      "Epoch 172/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6200 - precision: 0.6734 - recall: 0.8419 - acc: 0.6601 - auc: 0.6854 - val_loss: 0.6614 - val_precision: 0.5139 - val_recall: 0.7741 - val_acc: 0.5854 - val_auc: 0.6718\n",
      "Epoch 173/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6329 - precision: 0.6538 - recall: 0.8685 - acc: 0.6452 - auc: 0.6576 - val_loss: 1.0504 - val_precision: 0.4810 - val_recall: 0.8860 - val_acc: 0.5370 - val_auc: 0.6416\n",
      "Epoch 174/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6337 - precision: 0.6512 - recall: 0.8791 - acc: 0.6449 - auc: 0.6524 - val_loss: 0.6837 - val_precision: 0.4861 - val_recall: 0.8757 - val_acc: 0.5457 - val_auc: 0.6714\n",
      "Epoch 175/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6342 - precision: 0.6497 - recall: 0.8864 - acc: 0.6451 - auc: 0.6503 - val_loss: 0.6886 - val_precision: 0.4831 - val_recall: 0.8839 - val_acc: 0.5406 - val_auc: 0.6667\n",
      "Epoch 176/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6340 - precision: 0.6490 - recall: 0.8869 - acc: 0.6444 - auc: 0.6502 - val_loss: 0.6874 - val_precision: 0.4832 - val_recall: 0.8829 - val_acc: 0.5408 - val_auc: 0.6479\n",
      "Epoch 177/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6327 - precision: 0.6494 - recall: 0.8864 - acc: 0.6448 - auc: 0.6524 - val_loss: 0.6789 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6468\n",
      "Epoch 178/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6302 - precision: 0.6522 - recall: 0.8782 - acc: 0.6460 - auc: 0.6631 - val_loss: 1.0982 - val_precision: 0.4479 - val_recall: 0.9848 - val_acc: 0.4683 - val_auc: 0.6907\n",
      "Epoch 179/2000\n",
      "186145/186145 [==============================] - 7s 38us/sample - loss: 0.6339 - precision: 0.6548 - recall: 0.8592 - acc: 0.6437 - auc: 0.6527 - val_loss: 1.0788 - val_precision: 0.4763 - val_recall: 0.9028 - val_acc: 0.5285 - val_auc: 0.5596\n",
      "Epoch 180/2000\n",
      "186145/186145 [==============================] - 7s 38us/sample - loss: 0.6316 - precision: 0.6508 - recall: 0.8838 - acc: 0.6458 - auc: 0.6575 - val_loss: 0.9743 - val_precision: 0.4775 - val_recall: 0.9003 - val_acc: 0.5306 - val_auc: 0.6537\n",
      "Epoch 181/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6262 - precision: 0.6623 - recall: 0.8525 - acc: 0.6507 - auc: 0.6726 - val_loss: 0.9937 - val_precision: 0.4812 - val_recall: 0.8878 - val_acc: 0.5373 - val_auc: 0.6758\n",
      "Epoch 182/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6233 - precision: 0.6701 - recall: 0.8369 - acc: 0.6549 - auc: 0.6792 - val_loss: 0.6652 - val_precision: 0.9298 - val_recall: 0.0059 - val_acc: 0.5697 - val_auc: 0.6849\n",
      "Epoch 183/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6238 - precision: 0.6692 - recall: 0.8429 - acc: 0.6557 - auc: 0.6774 - val_loss: 0.6776 - val_precision: 0.4874 - val_recall: 0.8717 - val_acc: 0.5478 - val_auc: 0.6682\n",
      "Epoch 184/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6223 - precision: 0.6681 - recall: 0.8552 - acc: 0.6582 - auc: 0.6803 - val_loss: 0.9922 - val_precision: 0.4825 - val_recall: 0.8862 - val_acc: 0.5395 - val_auc: 0.644836 - precision: 0.6650 - recall: 0.8598 - acc: 0.\n",
      "Epoch 185/2000\n",
      "186145/186145 [==============================] - 7s 38us/sample - loss: 0.6220 - precision: 0.6685 - recall: 0.8520 - acc: 0.6577 - auc: 0.6804 - val_loss: 0.8597 - val_precision: 0.4789 - val_recall: 0.8976 - val_acc: 0.5331 - val_auc: 0.5764\n",
      "Epoch 186/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6310 - precision: 0.6528 - recall: 0.8747 - acc: 0.6457 - auc: 0.6587 - val_loss: 0.8875 - val_precision: 0.4814 - val_recall: 0.8874 - val_acc: 0.5377 - val_auc: 0.5505\n",
      "Epoch 187/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6229 - precision: 0.6668 - recall: 0.8610 - acc: 0.6585 - auc: 0.6791 - val_loss: 0.7187 - val_precision: 0.4835 - val_recall: 0.8835 - val_acc: 0.5412 - val_auc: 0.6815\n",
      "Epoch 188/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6224 - precision: 0.6707 - recall: 0.8457 - acc: 0.6583 - auc: 0.6813 - val_loss: 0.6489 - val_precision: 0.7169 - val_recall: 0.2538 - val_acc: 0.6338 - val_auc: 0.6849\n",
      "Epoch 189/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6199 - precision: 0.6791 - recall: 0.8311 - acc: 0.6630 - auc: 0.6865 - val_loss: 0.6950 - val_precision: 0.5376 - val_recall: 0.6957 - val_acc: 0.6094 - val_auc: 0.6729\n",
      "Epoch 190/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6275 - precision: 0.6645 - recall: 0.8437 - acc: 0.6506 - auc: 0.6716 - val_loss: 0.8763 - val_precision: 0.4938 - val_recall: 0.8447 - val_acc: 0.5581 - val_auc: 0.5904\n",
      "Epoch 191/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6328 - precision: 0.6509 - recall: 0.8818 - acc: 0.6453 - auc: 0.6540 - val_loss: 0.9373 - val_precision: 0.4813 - val_recall: 0.8923 - val_acc: 0.5373 - val_auc: 0.5712\n",
      "Epoch 192/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6306 - precision: 0.6528 - recall: 0.8786 - acc: 0.6468 - auc: 0.6588 - val_loss: 0.6839 - val_precision: 0.4735 - val_recall: 0.9213 - val_acc: 0.5227 - val_auc: 0.5619\n",
      "Epoch 193/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6301 - precision: 0.6571 - recall: 0.8755 - acc: 0.6511 - auc: 0.6645 - val_loss: 1.0027 - val_precision: 0.4610 - val_recall: 0.9492 - val_acc: 0.4978 - val_auc: 0.6012\n",
      "Epoch 194/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6278 - precision: 0.6611 - recall: 0.8468 - acc: 0.6476 - auc: 0.6688 - val_loss: 0.7850 - val_precision: 0.4671 - val_recall: 0.9378 - val_acc: 0.5102 - val_auc: 0.4642\n",
      "Epoch 195/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6318 - precision: 0.6497 - recall: 0.8845 - acc: 0.6446 - auc: 0.6559 - val_loss: 0.6823 - val_precision: 0.4927 - val_recall: 0.8498 - val_acc: 0.5564 - val_auc: 0.6729\n",
      "Epoch 196/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6308 - precision: 0.6514 - recall: 0.8793 - acc: 0.6452 - auc: 0.6598 - val_loss: 0.8180 - val_precision: 0.5033 - val_recall: 0.8167 - val_acc: 0.5720 - val_auc: 0.6608\n",
      "Epoch 197/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6303 - precision: 0.6521 - recall: 0.8782 - acc: 0.6458 - auc: 0.6603 - val_loss: 0.6619 - val_precision: 0.5348 - val_recall: 0.7068 - val_acc: 0.6071 - val_auc: 0.6743\n",
      "Epoch 198/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6281 - precision: 0.6568 - recall: 0.8664 - acc: 0.6482 - auc: 0.6674 - val_loss: 0.6769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6794\n",
      "Epoch 199/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6241 - precision: 0.6697 - recall: 0.8398 - acc: 0.6554 - auc: 0.6776 - val_loss: 0.6788 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6113\n",
      "Epoch 200/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6189 - precision: 0.6802 - recall: 0.8314 - acc: 0.6643 - auc: 0.6866 - val_loss: 1.0787 - val_precision: 0.4658 - val_recall: 0.9390 - val_acc: 0.5078 - val_auc: 0.5804\n",
      "Epoch 201/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6202 - precision: 0.6759 - recall: 0.8338 - acc: 0.6604 - auc: 0.6843 - val_loss: 0.8180 - val_precision: 0.4794 - val_recall: 0.8971 - val_acc: 0.5339 - val_auc: 0.5319\n",
      "Epoch 202/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6328 - precision: 0.6502 - recall: 0.8830 - acc: 0.6448 - auc: 0.6548 - val_loss: 0.6928 - val_precision: 0.4883 - val_recall: 0.8683 - val_acc: 0.5494 - val_auc: 0.6714\n",
      "Epoch 203/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6319 - precision: 0.6509 - recall: 0.8830 - acc: 0.6456 - auc: 0.6553 - val_loss: 0.6878 - val_precision: 0.4772 - val_recall: 0.9003 - val_acc: 0.5301 - val_auc: 0.6078\n",
      "Epoch 204/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6290 - precision: 0.6519 - recall: 0.8846 - acc: 0.6474 - auc: 0.6652 - val_loss: 0.6666 - val_precision: 0.4883 - val_recall: 0.8872 - val_acc: 0.5490 - val_auc: 0.6789\n",
      "Epoch 205/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6330 - precision: 0.6536 - recall: 0.8662 - acc: 0.6442 - auc: 0.6536 - val_loss: 0.6779 - val_precision: 0.4924 - val_recall: 0.8453 - val_acc: 0.5561 - val_auc: 0.6682\n",
      "Epoch 206/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6324 - precision: 0.6497 - recall: 0.8842 - acc: 0.6445 - auc: 0.6541 - val_loss: 0.6752 - val_precision: 0.4947 - val_recall: 0.8434 - val_acc: 0.5594 - val_auc: 0.6574\n",
      "Epoch 207/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6304 - precision: 0.6515 - recall: 0.8786 - acc: 0.6452 - auc: 0.6600 - val_loss: 0.6614 - val_precision: 0.5564 - val_recall: 0.6255 - val_acc: 0.6222 - val_auc: 0.6523\n",
      "Epoch 208/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6289 - precision: 0.6553 - recall: 0.8665 - acc: 0.6464 - auc: 0.6658 - val_loss: 0.6626 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6642\n",
      "Epoch 209/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6225 - precision: 0.6726 - recall: 0.8237 - acc: 0.6537 - auc: 0.6809 - val_loss: 0.7318 - val_precision: 0.4789 - val_recall: 0.8989 - val_acc: 0.5330 - val_auc: 0.6460\n",
      "Epoch 210/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6215 - precision: 0.6721 - recall: 0.8327 - acc: 0.6559 - auc: 0.6823 - val_loss: 0.8550 - val_precision: 0.4809 - val_recall: 0.8831 - val_acc: 0.5369 - val_auc: 0.4969\n",
      "Epoch 211/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6202 - precision: 0.6766 - recall: 0.8283 - acc: 0.6594 - auc: 0.6848 - val_loss: 0.6790 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6618\n",
      "Epoch 212/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6198 - precision: 0.6770 - recall: 0.8197 - acc: 0.6572 - auc: 0.6853 - val_loss: 0.8010 - val_precision: 0.5204 - val_recall: 0.7597 - val_acc: 0.5930 - val_auc: 0.6617\n",
      "Epoch 213/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6240 - precision: 0.6615 - recall: 0.8600 - acc: 0.6519 - auc: 0.6772 - val_loss: 0.6877 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.5204\n",
      "Epoch 214/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6206 - precision: 0.6766 - recall: 0.8228 - acc: 0.6577 - auc: 0.6852 - val_loss: 0.6764 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6666\n",
      "Epoch 215/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6172 - precision: 0.6838 - recall: 0.8155 - acc: 0.6630 - auc: 0.6900 - val_loss: 0.8266 - val_precision: 0.2982 - val_recall: 0.0645 - val_acc: 0.5296 - val_auc: 0.3447\n",
      "Epoch 216/2000\n",
      "186145/186145 [==============================] - 7s 38us/sample - loss: 0.6312 - precision: 0.6495 - recall: 0.8884 - acc: 0.6454 - auc: 0.6570 - val_loss: 1.0661 - val_precision: 0.4628 - val_recall: 0.9452 - val_acc: 0.5015 - val_auc: 0.6546\n",
      "Epoch 217/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6279 - precision: 0.6522 - recall: 0.8790 - acc: 0.6461 - auc: 0.6657 - val_loss: 0.7750 - val_precision: 0.4642 - val_recall: 0.9431 - val_acc: 0.5044 - val_auc: 0.6762\n",
      "Epoch 218/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6214 - precision: 0.6680 - recall: 0.8400 - acc: 0.6535 - auc: 0.6819 - val_loss: 0.6700 - val_precision: 0.5294 - val_recall: 0.7241 - val_acc: 0.6021 - val_auc: 0.6772\n",
      "Epoch 219/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6156 - precision: 0.6869 - recall: 0.8030 - acc: 0.6623 - auc: 0.6936 - val_loss: 0.6763 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6287\n",
      "Epoch 220/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6157 - precision: 0.6810 - recall: 0.8211 - acc: 0.6619 - auc: 0.6951 - val_loss: 0.6828 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.5885\n",
      "Epoch 221/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6156 - precision: 0.6854 - recall: 0.8092 - acc: 0.6627 - auc: 0.6940 - val_loss: 0.8694 - val_precision: 0.4819 - val_recall: 0.8864 - val_acc: 0.5385 - val_auc: 0.6490\n",
      "Epoch 222/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6247 - precision: 0.6570 - recall: 0.8633 - acc: 0.6476 - auc: 0.6746 - val_loss: 0.6751 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6042\n",
      "Epoch 223/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6105 - precision: 0.6982 - recall: 0.7921 - acc: 0.6698 - auc: 0.7026 - val_loss: 0.6792 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.5760\n",
      "Epoch 224/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6136 - precision: 0.6799 - recall: 0.8292 - acc: 0.6633 - auc: 0.6976 - val_loss: 0.6976 - val_precision: 0.4808 - val_recall: 0.8880 - val_acc: 0.5367 - val_auc: 0.6786\n",
      "Epoch 225/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6255 - precision: 0.6688 - recall: 0.8344 - acc: 0.6527 - auc: 0.6773 - val_loss: 0.9085 - val_precision: 0.4790 - val_recall: 0.8976 - val_acc: 0.5332 - val_auc: 0.5424\n",
      "Epoch 226/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6326 - precision: 0.6510 - recall: 0.8759 - acc: 0.6438 - auc: 0.6546 - val_loss: 1.1598 - val_precision: 0.4774 - val_recall: 0.9004 - val_acc: 0.5304 - val_auc: 0.6275\n",
      "Epoch 227/2000\n",
      "186145/186145 [==============================] - 7s 37us/sample - loss: 0.6280 - precision: 0.6562 - recall: 0.8649 - acc: 0.6470 - auc: 0.6673 - val_loss: 0.6759 - val_precision: 0.4856 - val_recall: 0.8769 - val_acc: 0.5447 - val_auc: 0.6743\n",
      "Epoch 228/2000\n",
      "186145/186145 [==============================] - 7s 38us/sample - loss: 0.6182 - precision: 0.6698 - recall: 0.8513 - acc: 0.6589 - auc: 0.6884 - val_loss: 0.6738 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.6259\n",
      "Epoch 229/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6176 - precision: 0.6754 - recall: 0.8341 - acc: 0.6600 - auc: 0.6898 - val_loss: 0.7483 - val_precision: 0.2303 - val_recall: 0.0238 - val_acc: 0.5432 - val_auc: 0.4312\n",
      "Epoch 230/2000\n",
      "186145/186145 [==============================] - 9s 49us/sample - loss: 0.6065 - precision: 0.6938 - recall: 0.8239 - acc: 0.6761 - auc: 0.7097 - val_loss: 0.7025 - val_precision: 0.4732 - val_recall: 0.8299 - val_acc: 0.5266 - val_auc: 0.5569\n",
      "Epoch 231/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6166 - precision: 0.6792 - recall: 0.8273 - acc: 0.6619 - auc: 0.6925 - val_loss: 0.7199 - val_precision: 0.2769 - val_recall: 0.1001 - val_acc: 0.4975 - val_auc: 0.4859\n",
      "Epoch 232/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6074 - precision: 0.7049 - recall: 0.7921 - acc: 0.6763 - auc: 0.7068 - val_loss: 0.7637 - val_precision: 0.5167 - val_recall: 0.7295 - val_acc: 0.5877 - val_auc: 0.5883\n",
      "Epoch 233/2000\n",
      "186145/186145 [==============================] - 7s 35us/sample - loss: 0.6149 - precision: 0.6808 - recall: 0.8232 - acc: 0.6623 - auc: 0.6952 - val_loss: 0.8053 - val_precision: 0.3344 - val_recall: 0.0115 - val_acc: 0.5624 - val_auc: 0.3390\n",
      "Epoch 234/2000\n",
      "186145/186145 [==============================] - 7s 35us/sample - loss: 0.6324 - precision: 0.6539 - recall: 0.8506 - acc: 0.6402 - auc: 0.6549 - val_loss: 0.7553 - val_precision: 0.4897 - val_recall: 0.8726 - val_acc: 0.5514 - val_auc: 0.5436\n",
      "Epoch 235/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6253 - precision: 0.6581 - recall: 0.8585 - acc: 0.6474 - auc: 0.6735 - val_loss: 0.7530 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5673 - val_auc: 0.4657\n",
      "Epoch 236/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6349 - precision: 0.6535 - recall: 0.8644 - acc: 0.6437 - auc: 0.6552 - val_loss: 1.2880 - val_precision: 0.1352 - val_recall: 0.0048 - val_acc: 0.5561 - val_auc: 0.4402\n",
      "Epoch 237/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6525 - precision: 0.6370 - recall: 0.9271 - acc: 0.6392 - auc: 0.5733 - val_loss: 1.5184 - val_precision: 0.1301 - val_recall: 0.0083 - val_acc: 0.5470 - val_auc: 0.4242\n",
      "Epoch 238/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6499 - precision: 0.6407 - recall: 0.9108 - acc: 0.6400 - auc: 0.5848 - val_loss: 0.7462 - val_precision: 0.4438 - val_recall: 0.9893 - val_acc: 0.4588 - val_auc: 0.5212\n",
      "Epoch 239/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6468 - precision: 0.6443 - recall: 0.9002 - acc: 0.6420 - auc: 0.6033 - val_loss: 0.7915 - val_precision: 0.4440 - val_recall: 0.9889 - val_acc: 0.4595 - val_auc: 0.5269\n",
      "Epoch 240/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6442 - precision: 0.6466 - recall: 0.8903 - acc: 0.6422 - auc: 0.6205 - val_loss: 0.8202 - val_precision: 0.4445 - val_recall: 0.9885 - val_acc: 0.4605 - val_auc: 0.5048\n",
      "Epoch 241/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6425 - precision: 0.6483 - recall: 0.8855 - acc: 0.6430 - auc: 0.6308 - val_loss: 0.7817 - val_precision: 0.4549 - val_recall: 0.9620 - val_acc: 0.4847 - val_auc: 0.5330\n",
      "Epoch 242/2000\n",
      "186145/186145 [==============================] - 6s 34us/sample - loss: 0.6413 - precision: 0.6476 - recall: 0.8874 - acc: 0.6427 - auc: 0.6331 - val_loss: 0.7213 - val_precision: 0.2915 - val_recall: 0.1146 - val_acc: 0.4963 - val_auc: 0.3819\n",
      "Epoch 243/2000\n",
      "186145/186145 [==============================] - 6s 33us/sample - loss: 0.6400 - precision: 0.6471 - recall: 0.8865 - acc: 0.6418 - auc: 0.6375 - val_loss: 0.8821 - val_precision: 0.2513 - val_recall: 0.1218 - val_acc: 0.4630 - val_auc: 0.3527\n",
      "Epoch 244/2000\n",
      "186145/186145 [==============================] - 6s 32us/sample - loss: 0.6386 - precision: 0.6491 - recall: 0.8802 - acc: 0.6426 - auc: 0.6401 - val_loss: 0.7628 - val_precision: 0.4475 - val_recall: 0.9856 - val_acc: 0.4672 - val_auc: 0.5983\n",
      "Epoch 245/2000\n",
      "186145/186145 [==============================] - 6s 31us/sample - loss: 0.6367 - precision: 0.6481 - recall: 0.8880 - acc: 0.6435 - auc: 0.6455 - val_loss: 0.8234 - val_precision: 0.4564 - val_recall: 0.9600 - val_acc: 0.4879 - val_auc: 0.5686\n",
      "Epoch 246/2000\n",
      "186145/186145 [==============================] - 6s 31us/sample - loss: 0.6384 - precision: 0.6488 - recall: 0.8829 - acc: 0.6430 - auc: 0.6408 - val_loss: 0.7738 - val_precision: 0.4489 - val_recall: 0.9117 - val_acc: 0.4775 - val_auc: 0.4514\n",
      "Epoch 247/2000\n",
      "186145/186145 [==============================] - 6s 32us/sample - loss: 0.6376 - precision: 0.6484 - recall: 0.8848 - acc: 0.6430 - auc: 0.6415 - val_loss: 0.8607 - val_precision: 0.4555 - val_recall: 0.9618 - val_acc: 0.4861 - val_auc: 0.5722\n",
      "Epoch 248/2000\n",
      "186145/186145 [==============================] - 6s 32us/sample - loss: 0.6359 - precision: 0.6497 - recall: 0.8820 - acc: 0.6439 - auc: 0.6460 - val_loss: 0.9374 - val_precision: 0.4547 - val_recall: 0.9627 - val_acc: 0.4843 - val_auc: 0.4931\n",
      "Epoch 249/2000\n",
      "186145/186145 [==============================] - 6s 32us/sample - loss: 0.6384 - precision: 0.6474 - recall: 0.8889 - acc: 0.6429 - auc: 0.6397 - val_loss: 0.7848 - val_precision: 0.4502 - val_recall: 0.9812 - val_acc: 0.4735 - val_auc: 0.5835\n",
      "Epoch 250/2000\n",
      "186145/186145 [==============================] - 6s 31us/sample - loss: 0.6403 - precision: 0.6472 - recall: 0.8909 - acc: 0.6431 - auc: 0.6373 - val_loss: 0.6671 - val_precision: 0.5152 - val_recall: 0.7681 - val_acc: 0.5870 - val_auc: 0.6617\n",
      "Epoch 251/2000\n",
      "186145/186145 [==============================] - 6s 31us/sample - loss: 0.6352 - precision: 0.6493 - recall: 0.8853 - acc: 0.6442 - auc: 0.6485 - val_loss: 0.6683 - val_precision: 0.4860 - val_recall: 0.8734 - val_acc: 0.5455 - val_auc: 0.6632\n",
      "Epoch 252/2000\n",
      "186145/186145 [==============================] - 6s 32us/sample - loss: 0.6346 - precision: 0.6504 - recall: 0.8853 - acc: 0.6457 - auc: 0.6515 - val_loss: 0.7269 - val_precision: 0.4771 - val_recall: 0.8994 - val_acc: 0.5300 - val_auc: 0.6673\n",
      "Epoch 253/2000\n",
      "186145/186145 [==============================] - 6s 32us/sample - loss: 0.6343 - precision: 0.6510 - recall: 0.8825 - acc: 0.6456 - auc: 0.6521 - val_loss: 0.8361 - val_precision: 0.4659 - val_recall: 0.9388 - val_acc: 0.5080 - val_auc: 0.4851\n",
      "Epoch 254/2000\n",
      " 87040/186145 [=============>................] - ETA: 3s - loss: 0.6336 - precision: 0.6552 - recall: 0.8708 - acc: 0.6460 - auc: 0.6512"
     ]
    }
   ],
   "source": [
    "model_dt = clf\n",
    "#model_dt = make_pipeline(OrdinalEncoder(), SimpleImputer(strategy='median'), QuantileTransformer(output_distribution='uniform'), clf)\n",
    "#model_dt = make_pipeline(OrdinalEncoder(), clf)#SimpleImputer(strategy='mean'), clf)\n",
    "model_dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d88ef402326516bce18861e13161307746d0cc4701c0d03d978e88a7ff5f9cba"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
